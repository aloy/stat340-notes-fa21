<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Hierarchical models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Stat 340: Bayesian Statistics" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="assets/css/my-theme.css" type="text/css" />
    <link rel="stylesheet" href="assets/css/my-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, left, middle

# Hierarchical models

### Stat 340: Bayesian Statistics


---

## Example: ELS math scores

- 2002 Educational Longitudinal Study (ELS)

- Survey from schools across the United States

- Data are collected by sampling schools and then sampling students within each selected school

- We'll focus on 10th grade math scores from a sample of 10 schools

- Math tests contained items in arithmetic, algebra, geometry, data/probability, and advanced topics were divided into process categories of skill/knowledge, understanding/ comprehension, and problem solving

---

## ELS math scores

.left-wide[
&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-1-1.svg" width="98%" /&gt;
]

.right-narrow[
Possible questions:

- What’s the typical math score?

- To what extent do scores vary from school to school?

- For any single school, how much might scores vary from student to student?
]

---

## Possible analysis strategies

.bold[Complete pooling (combined estimates)]&lt;br&gt;
Ignore schools and lump all students together

--

.bold[No pooling (separate groups)]&lt;br&gt;
Separately analyze each school and assume that one school’s data doesn’t contain valuable information about another school

--

.bold[Partial pooling (compromise estimates)]&lt;br&gt;
Acknowledge the grouping structure, so that even though schools differ in performance, they might share valuable information about each other and about the broader population of schools

---

## What have we seen so far?

- Completely pooled model does not acknowledge differences between schools

- No pooled model acknowledges that some schools tend to score higher than others

- No pooled model ignores data on one school when learning about the typical score of another

- No pooled model cannot be generalized to schools outside our sample

---

## Hierarchical model

Let's compromise between the the complete pooled and no pooled models 

How? By using a *two-stage prior* specification

---

#### Hierarchical model specification for JAGS

.code100[

```r
modelString &lt;-"model {

## sampling
for (i in 1:N){
   y[i] ~ dnorm(mu_j[school[i]], invsigma2)
}

## priors
for (j in 1:J){
   mu_j[j] ~ dnorm(mu, invtau2)
}

invsigma2 ~ dgamma(a_s, b_s)
sigma &lt;- sqrt(pow(invsigma2, -1))

## hyperpriors
mu ~ dnorm(mu0, g0)
invtau2 ~ dgamma(a_t, b_t)
tau &lt;- sqrt(pow(invtau2, -1))
}
"
```
]

---

#### Define the data and prior parameters



```r
y &lt;- sub_school$mathscore      
school &lt;- sub_school$school
N &lt;- length(y)  
J &lt;- length(unique(school)) 
the_data &lt;- list(y = y, school = school, 
                 N = N, J = J,
                 mu0 = 50, g0 = .04,  # prior parameters
                 a_t = 1, b_t = .01,  # hyperparameters
                 a_s = 1, b_s = .01)  # hyperparameters
```

---

#### Run MCMC


```r
posterior &lt;- run.jags(
  modelString,
  n.chains = 1,
  data = the_data,
  monitor = c("mu", "tau", "mu_j", "sigma"),
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  silent.jags = TRUE
)
```

---

.code100[

```r
print(posterior, digits = 3)  
```

```
## 
## JAGS model summary statistics from 5000 samples (adapt+burnin = 6000):
##                                                                             
##         Lower95 Median Upper95 Mean    SD Mode  MCerr MC%ofSD SSeff    AC.10
## mu         45.8   51.8    57.6 51.7  3.01 51.8 0.0453     1.5  4411 -0.00241
## tau        3.71   7.68    14.3  8.3  3.12  6.9 0.0553     1.8  3189   0.0281
## mu_j[1]    35.9   39.9    43.9 39.9  2.04 40.3 0.0315     1.5  4184 -0.00417
## mu_j[2]    43.9   47.3    50.7 47.3  1.73 47.5  0.025     1.4  4783   0.0104
## mu_j[3]    53.3   56.6      60 56.6   1.7 56.6 0.0247     1.4  4763 -0.00338
## mu_j[4]    52.7   57.2    61.8 57.2  2.32 57.1 0.0339     1.5  4708  0.00782
## mu_j[5]    54.2     62    69.2   62  3.86 61.9 0.0624     1.6  3826   0.0131
## sigma      6.71   7.95    9.37    8 0.693  7.9 0.0105     1.5  4318 -0.00179
##             
##         psrf
## mu        --
## tau       --
## mu_j[1]   --
## mu_j[2]   --
## mu_j[3]   --
## mu_j[4]   --
## mu_j[5]   --
## sigma     --
## 
## Total time taken: 0.8 seconds
```
]

---


```r
mcmc_intervals(posterior$mcmc, regex_pars = "mu")
```

&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-6-1.svg" width="85%" /&gt;

---

## Hierarchical predictions vs. sample means

&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-7-1.svg" width="90%" style="display: block; margin: auto;" /&gt;

---
background-image: url(img/bayesrule-hierarchical-varcomp.png)
background-size: 100%
background-position: middle
## Comparing sources of variability

.flex[
.left-col[
.hidden[X]between &gt; within
]

.middle-col[
.hidden[XX]between `\(\approx\)` within
]

.right-col[
.hidden[XXXX]between &lt; within
]
]

---

## Within-group (intraclass) correlation

Suppose we're in the situation where between group variability is much larger than within group variability

.pull-left[
&lt;img src="img/bayesrule-hierarchical-varcomp2.png" width="1163" /&gt;
]

.pull-right[
- Two observations within the same group are more similar than two observations from different groups

- Observations within the same group are correlated &lt;br&gt;(generally true, easier to see in this extreme case)
]

---

## Full ELS data set

&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-9-1.svg" width="100%" /&gt;

- 100 schools from urban settings in the full data set

- Sample sizes range from 4 to 32 students


```r
full_data &lt;- list(
  y = els$mathscore, 
  school = els$school, 
  N = nrow(els), 
  J = nlevels(els$school),
  mu0 = 50, g0 = .04,  # prior parameters
  a_t = 1, b_t = .01,  # hyperparameters
  a_s = 1, b_s = .01   # hyperparameters
)

posterior_full &lt;- run.jags(
  modelString,
  n.chains = 1,
  data = full_data,
  monitor = c("mu", "tau", "mu_j", "sigma"),
  adapt = 1000,
  burnin = 5000,
  sample = 5000,
  silent.jags = TRUE
)

post_intervals &lt;- mcmc_intervals_data(posterior_full$mcmc, regex_pars = "mu")
```

---

## ELS intraclass correlation



```r
draws &lt;- posterior_full$mcmc[[1]]
icc &lt;- draws[,"tau"]^2 / (draws[,"tau"]^2 + draws[,"sigma"]^2)
```

.left-wide[
&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-12-1.svg" width="95%" style="display: block; margin: auto;" /&gt;
]

.right-narrow[
90% credible interval:

```
##    5%   95% 
## 0.165 0.264
```
]

---

## ELS global mean

Inference for the global parameters proceeds as always

.left-wide[
&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-14-1.svg" width="95%" style="display: block; margin: auto;" /&gt;
]

.right-narrow[
90% credible interval:

```
##    5%   95% 
## 47.27 48.97
```
]

---

## Inference for group-specific means

There are often *a lot* of parameters to manage for group-specific inference

&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-16-1.svg" width="100%" /&gt;

---

class: inverse

## Your turn

School 3 and 17 have roughly the same posterior mean, but substantially different credible interval widths

Discuss with a neighbor why you think this difference occurs.

![](18-hierarchical-normal2_files/figure-html/unnamed-chunk-17-1.svg)&lt;!-- --&gt;

<div class="countdown" id="timer_61820d69" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">02</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---

## Prediction for observed group

Suppose we want to make a prediction for school 13, then we need a posterior predictive distribution


```r
# Work with a data frame
post_df &lt;- as.data.frame(posterior_full$mcmc[[1]])

# Create the posterior predictive via simulation
pp_school13 &lt;- post_df %&gt;%
  select(mu_j = "mu_j[13]", sigma) %&gt;%               # select relevant cols
  mutate(y_pred = rnorm(nrow(post_df), mu_j, sigma)) # simulate a new obs.

# Check it out
head(pp_school13, 3)
```

```
##          mu_j    sigma   y_pred
## 6001 40.65318 9.220115 17.68957
## 6002 42.60158 8.925333 50.84780
## 6003 40.14430 9.230251 37.15336
```

---

## Prediction for unobserved group

Suppose we want to make a prediction for a school we didn't observe, let's call it school 101


```r
# Create the posterior predictive via simulation
pp_school101 &lt;- post_df %&gt;%
  select(mu, tau, sigma) %&gt;%                   # select global params
  mutate(
    mu_j = rnorm(nrow(post_df), mu, tau),      # generate mu_j
    y_pred = rnorm(nrow(post_df), mu_j, sigma) # generate y
  ) 

# Check it out
head(pp_school101, 3)
```

```
##            mu      tau    sigma     mu_j   y_pred
## 6001 48.76941 4.738100 9.220115 41.14001 51.18014
## 6002 48.92055 4.394430 8.925333 47.45571 40.40386
## 6003 47.77900 4.468838 9.230251 42.37273 43.12030
```

---

## How do the predictions compare?

&lt;img src="18-hierarchical-normal2_files/figure-html/unnamed-chunk-21-1.svg" width="70%" /&gt;

---

## Prior distributions for between group variance

For `\(J &gt; 5\)`, common choices include 

- improper uniform (either on the variance or log variance scale), but not allowed in JAGS 

- proper uniform (with large upper bound) - `dunif`

- proper normal (with large variance `\(\rightarrow\)` small precision) - `dnorm`

.footnote[Gelman (2006). Prior distributions for variance parameters in hierarchical models." *Bayesian Analysis*. 1 (3) 515 - 534.]

---

## Prior distributions for between group variance

For `\(J &lt; 5\)`, or in a situation where more prior information is helpful, 

- Use half-*t* distribution

- Half-Cauchy with reasonably large scale parameter - `dhalfcauchy(sigma)`


.footnote[Gelman (2006). Prior distributions for variance parameters in hierarchical models." *Bayesian Analysis*. 1 (3) 515 - 534.]

---

## Prior distributions for between group variance

- In general, Gelman (2006) does not recommend the `\({\rm Gamma}(\epsilon, \epsilon)\)` family of noninformative prior distributions because the inferences are often sensitive to the choice of `\(\epsilon\)`.

- Different than advice in the textbook

.footnote[Gelman (2006). Prior distributions for variance parameters in hierarchical models." *Bayesian Analysis*. 1 (3) 515 - 534.]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"output": null,
"ratio": "16:9",
"highlightStyle": "solarized-light",
"highlightLanguage": ["r", "css", "yaml"],
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
