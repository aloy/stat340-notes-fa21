---
title: "Using a continuous prior distribution"
author: "Stat 340: Bayesian Statistics"
output:
  xaringan::moon_reader:
    css: ["default", "assets/css/my-theme.css", "assets/css/my-fonts.css", "hygge"]
    seal: false
    lib_dir: libs
    nature:
      output:
      ratio: '16:9'
      highlightStyle: solarized-light
      highlightLanguage: ["r", "css", "yaml"]
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(dev = 'svg')
library(gridExtra)
library(dplyr)
library(ggplot2)
library(countdown)
library(fontawesome)
library(xaringanthemer)
library(ggthemes)
library(janitor)
yt <- 0
```

class: title-slide, left, middle
background-image: url(img/densities.png)
background-position: right
background-size: 45%

.left-wide[
# `r rmarkdown::metadata$title`

### `r rmarkdown::metadata$author`
]
---
class: middle

# 1. Continuous prior

# 2. Posterior analysis

# 3. Prediction

# (Problem topics 1-4)

---

## Blindsight design, redux


**Data:** N N N N **<font color = "tomato">B B</font>** N N N  **<font color = "tomato">B</font>** N N N N N N N (14 Ns; **<font color = "tomato">3 Bs</font>**)

<br>

**Data model (likelihood):**

Some true proportion of guesses, $p$

Toss a coin with probability of heads, $p$

<br>

**Belief about $p$:**

Uniform over (0, 1)

---
background-image: url(img/Beta-priors.png)
background-position: right
background-size:contain
## Beta distribution


- $f(x|a, b) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1}$

- Parameter space: $a>0$, $b>0$

- Support: $0 < x < 1$

.footnote[Image credit: Probability and Bayesian Modeling]

---
class: middle
background-image: url("figs/002x006.png")
background-position: left
background-size: 50%

.pull-right[

# "The prior is proportional to the prior times the likelihood"
]

---
class: inverse

# Your turn `r (yt <- yt + 1)`

- Work with your neighbors

- Work through the R code to simulate kernels of the beta distribution

- You can copy/paste the code from the course webpage

- Develop your understanding of the kernel of a distribution


```{r echo=FALSE}
countdown(minutes = 5)
```

---
class: inverse

# Your turn `r (yt <- yt + 1)`


- Work with your neighbors

- Derive the posterior

- Are you working with a conjugate family?

```{r echo=FALSE}
countdown(minutes = 5)
```


---

## Posterior analysis

.pull-left[

> To a Bayesian, the best information one can ever have about $\theta$ is to know the posterior density.
>
> — Christensen, et al; *Bayesian Ideas and Data Analysis*, p. 31
]



```{r echo=FALSE, fig.height = 3, fig.width = 4, out.width = '50%'}
df <- tibble(theta = c(0, 1)) 

df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = 15, shape2 = 17 - 14 + 1)) +
  labs(x = "p", y = "density") +
  theme_minimal()
```


---

## Point estimates

.pull-left[
-  <strong>Posterior mean</strong> </font>

- <font color = "#E69F00"> <strong>Posterior median </strong></font>

- <font color = "#56B4E9"> <strong>Posterior mode </strong></font>  <br> i.e. *maximum a posteriori* (MAP) estimate

]


```{r echo=FALSE, fig.height = 3, fig.width = 4, out.width = '50%'}
a <- 14 + 1
b <- 17 - 14 + 1
post_mean <- a / (a + b)
post_mode <- (a - 1) / (a + b - 2)
post_median <- qbeta(0.5, shape1 = a, shape2 = b)
df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_point(aes(x = post_mean, y = dbeta(post_mean, shape1 = a, shape2 = b), color = "mean")) +
  geom_point(aes(x = post_mode, y = dbeta(post_mode, shape1 = a, shape2 = b), color = "mode")) +
  geom_point(aes(x = post_median, y = dbeta(post_median, shape1 = a, shape2 = b), color = "median")) +
  labs(x = "p", y = "density") +
  scale_color_colorblind("Estimate") +
  theme_minimal() +
  theme(legend.position = "top")
```


---

## Credible intervals

```{r ci1, echo=FALSE, fig.height = 2.4, fig.width = 3.5, out.width = '40%', fig.align='center'}
ci1 <- df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_area(stat = "function", fun = dbeta, args = list(shape1 = a, shape2 = b), 
            xlim = c(qbeta(0.055, a, b), qbeta(1 - 0.055, a, b)), alpha = 0.5, fill = "steelblue") +
  labs(x = expression(p), y = "density",
       title = "89% equal-tailed credible interval") +
  scale_color_viridis_d("Estimate") +
  theme_minimal() +
  theme(legend.position = "top")
ci1
```

```{r}
# q*() functions calculate quantiles from the specified distribution
c(lower = qbeta(0.055, 15, 4), upper = qbeta(1 - 0.055, 15, 4))
```

---

## Credible intervals are not unique

Here are three 89% credible intervals

<br>

```{r ci2, echo=FALSE, fig.height = 2, fig.width = 7, out.width = '100%'}
ci1 <- ci1 + ggtitle(expression("(q"[0.055]*', q'[0.945]*")"))

ci2 <- df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_area(stat = "function", fun = dbeta, args = list(shape1 = a, shape2 = b), 
            xlim = c(qbeta(0.00001, a, b), qbeta(0.89, a, b)), alpha = 0.5, fill = "steelblue") +
  labs(x = expression(theta), y = "density",
       title = expression("(q"[0.00]*', q'[0.89]*")")) +
  scale_color_viridis_d("Estimate") +
  theme_minimal() +
  theme(legend.position = "top")

ci3 <- df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_area(stat = "function", fun = dbeta, args = list(shape1 = a, shape2 = b), 
            xlim = c(qbeta(0.11, a, b), qbeta(0.999999999, a, b)), alpha = 0.5, fill = "steelblue") +
  labs(x = expression(theta), y = "density",
       title = expression("(q"[0.11]*', q'[100]*")")) +
  scale_color_viridis_d("Estimate") +
  theme_minimal() +
  theme(legend.position = "top")

grid.arrange(ci1, ci2, ci3, ncol = 3)
```

---

## Testing a hypothesis

.large[
Suppose the researchers were interested in testing


.pull-left[

$H_0: p \le 0.5$

${\rm P}(p \le 0.5 | Y = 14) = `r round(pbeta(0.5, a, b), 3)`$

```{r, echo=FALSE, fig.width = 3.5, fig.height = 2.5}

df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_area(stat = "function", fun = dbeta, args = list(shape1 = a, shape2 = b), 
            xlim = c(0, 0.5), alpha = 0.5, fill = "steelblue") +
  labs(x = "p", y = "density") +
  theme_minimal() +
  theme(legend.position = "top")
```

]

.pull-right[

$H_1: p > 0.5$

${\rm P}(p > 0.5 | Y = 14) = `r 1 - round(pbeta(0.5, a, b), 3)`$

```{r, echo=FALSE, fig.width = 3.5, fig.height = 2.5}
df %>%
  ggplot(aes(x = theta)) +
  stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b)) +
  geom_area(stat = "function", fun = dbeta, args = list(shape1 = a, shape2 = b), 
            xlim = c(0.5, 1), alpha = 0.5, fill = "steelblue") +
  labs(x = "p", y = "density") +
  theme_minimal() +
  theme(legend.position = "top")
```
]

]

---

## Predicting a new observation

To make predictions, we need to work with the **posterior predictive** distribution:

\begin{align*}
f(\widetilde{Y} = \widetilde{y} | Y = y) &= \int_0^1 f(\widetilde{Y} = \widetilde{y}, p | Y = y) dp\\
  &= \int_0^1 f(\widetilde{Y} = \widetilde{y} | p, Y = y) \pi(p | Y = y) dp\\
  &= \int_0^1 f(\widetilde{Y} = \widetilde{y} | p) \pi(p | Y = y) dp
\end{align*}

.footnote[See Appendix B for algebraic work specific to the Binomial distribution]


---

## Monte Carlo simulation for prediction

Suppose we wish to make predictions for a new set of 20 "guesses" made by PS

Posterior predictive 
$$f(\widetilde{Y} = \widetilde{y} | Y = 14) = \displaystyle \int_0^1 f(\widetilde{Y} = \widetilde{y} | p) \pi(p | Y = 14) dp$$

Integration via simulation:

```{r echo=FALSE}
set.seed(1234)
```


```{r}
n <- 20   # No. of new binomial trials
S <- 1000 # No. simulations
sim_p <- rbeta(S, 15, 4)
sim_y <- rbinom(S, size = n, prob = sim_p)
```

---

.left-wide[
## Posterior predictive distribution
```{r echo=FALSE, echo=FALSE, fig.height = 2.5, fig.width = 4, out.width = "95%"}
data.frame(preds = sim_y) %>%
  ggplot() +
  geom_bar(mapping = aes(x = preds), width = .5, alpha = 0.7) +
  xlim(c(0, 21)) +
  labs(x = "Number of non-burning guesses") +
  theme_minimal()
```
]

.right-narrow[
```{r}
janitor::tabyl(sim_y)
```
]

---

## Prediction intervals

How can we construct an 89% prediction interval?

Put in the most likely values until the probability is **at least** 0.89

```{r}
post_pred_dsn <- janitor::tabyl(sim_y)[, -2]
LearnBayes::discint(post_pred_dsn, prob = 0.89)
```

---
class: inverse 

# Your turn `r (yt <- yt + 1)`

Let  $p$ denote the proportion of U.S. adults that do not believe in climate change. Of 1000 survey respondents, 150 responded that it was "not real at all".

1. Using a Beta(1, 2) prior distribution, what is the posterior distribution of $p$?

2. Simulate 1000 draws from the posterior distribution.

3. Use your simulated draws to calculate a 93% credible interval
 equal-tailed for $p$. Interpret this interval in context.

4. Suppose you were to survey 100 more adults. Approximate the probability that at least 20 of the 100 people don’t believe in climate change.

.footnote[https://thepulseofthenation.com/, data from September 2017. ]