\documentclass[11pt]{article}

\usepackage{amsmath,graphicx}
\usepackage[margin=.9in]{geometry}
\usepackage{parskip} % no indentation
\usepackage{array}
% \usepackage{enumitem}
\usepackage{enumerate}
\usepackage{changepage} % for adjustwidth
\usepackage{array}
\usepackage{booktabs}
\usepackage{multicol}

\begin{document}

{\Large \bf Introduction to Markov Chains}\\[1ex]
{\large \bf Stat 340, Fall 2021}


{\bf Example.\footnote{Source: Introduction to Stochastic Processes with R by Bob Dobrow}}
University administrators have developed a Markov chain model to simulate graduation rates at their school. Students might drop out, repeat a year, or move on to the next year. Below is a graph representing the possible transitions that students can make at the university. Probabilities are listed next to each possible path, and only paths with positive probability are drawn.

<<echo=FALSE, out.width="90%", fig.align='center'>>=
knitr::include_graphics("figs/graduation_mc.jpg")
@



{\bf Your turn:}

\begin{enumerate}

\item What's the probability that a student who drops out will re-enroll?\\[0.25in]

\item What's the probability that a senior will graduate?\\[0.25in]

\item Does that probability depend on how many years it took them to achieve senior class standing?\\[0.25in]

\end{enumerate}


{\bf Definition: Markov chain}

A sequence of random variables, $X_0, X_1, X_2, \ldots$, taking values in the \emph{state space} $\lbrace 1, \ldots, M \rbrace$ is called a Markov chain if for all $n \ge 0$\\[0.9in]

Remarks:

\begin{itemize}
 
\item Think of $X_n$ as the state of the system at (discrete) time $n$. 

\item $q_{ij}$ is the \emph{transition probability} from state $i$ to state $j$.

\end{itemize}

\clearpage

{\bf Definition: Transition matrix}

A Markov chain can be represented by an $M \times M$ matrix of the probabilities $Q = \left( q_{ij} \right)$

\bigskip

where the rows represent the

\bigskip
and the columns represent the


\bigskip

{\bf Your turn:} Write down the $6 \times 6$ transition matrix for the university graduation rate Markov chain model.\\[3in]


\begin{enumerate}
\setcounter{enumi}{3}
\item Should the probabilities within each row sum to 1?\\[0.5in]
\item Should the probabilities within each row sum to 1?\\[0.6in]
\end{enumerate}

{\bf Calculating probabilities using the transition matrix}

If we know the transition matrix, $Q$, then we can derive the probability that a student goes from state $i$ to state $j$ in some given number of steps.

\bigskip
\bigskip

One-step transition probability: $P(X_{n+1} = j | X_n = i ) =$

\clearpage

Two-step transition probability: $P(X_{n+2} = j | X_n = i ) =$\\[2.5in]

$m$-step transition probability: $P(X_{n+m} = j | X_n = i ) =$

\bigskip
\bigskip

{\bf Marginal distribution of $\bf X_n$}

Suppose that at time $n$, $X_n$ has PMF given by ${\bf s} = (s_1, s_2, \ldots, s_m)$ where $s_i = P(X_n = i)$.\\

We can use the law of total probability to derive the PMF of $X_{n+1}$:\\[2.5in]

{\bf Uses of Markov Chains}\\

\begin{enumerate}
\item Use a Markov chain model, if your Markov chain is a reasonable abstraction of reality.\\

\item {\bf Markov Chain Monte Carlo (MCMC)}. Synthetically construct a Markov chain that is {\emph known} to converge to the distribution of interest.\\
\end{enumerate}

Not all Markov chains will converge to a single distribution, so we need a few more concepts before we can explore MCMC.

\clearpage

{\bf Classification of states}

{\bf Definition:} A state is {\bf recurrent} if starting there, the chain has probability 1 of returning to that state.\\

{\bf Definition:} A state that is not recurrent is {\bf transient}.\\

{\bf Definition:} If it's possible to get from any state to any state in a chain (with positive probability) \emph{in a finite number of steps}, then  it is {\bf irreducible}.\\

{\bf Definition:} A chain that is not irreducible is {\bf reducible}.\\


{\bf Your turn:} Assume that each of the Markov chains given below have uniform transition probabilities. For each Markov chain

\begin{enumerate}[i.]
\item Classify the chain as reducible or irreducible
\item Identify the transient states
\item Identify the recurrent states
\end{enumerate}

\begin{multicols}{2}

<<echo=FALSE>>=
knitr::include_graphics("figs/mc1.png")
@

\vspace{1.15in}

<<echo=FALSE>>=
knitr::include_graphics("figs/mc2.png")
@

\vspace{1.15in}

<<echo=FALSE>>=
knitr::include_graphics("figs/mc3.png")
@

\vspace{1.75in}

<<echo=FALSE, out.width = "60%", fig.align='center'>>=
knitr::include_graphics("figs/mc4.png")
@

\vspace{1.15in}

\end{multicols}

\clearpage


{\bf Long-run behavior}

{\bf Definition.} For irreducible, aperiodic Markov chains, the fraction of the time spent in each of the recurrent states is given by the {\bf stationary distribution.} (a.k.a. steady state)

\bigskip

${\bf s} = (s_1, s_2, \ldots, s_m)$ is a stationary distribution if

\bigskip
\bigskip

{\bf Key result:}  A Markov chain which starts out with a stationary distribution will stay in the stationary distribution forever.

\bigskip
\bigskip

{\bf Theorem.} For any irreducible Markov chain:

\begin{enumerate}
\item A stationary distribution exists.

\item The stationary distribution is unique.

\item $s_i = 1/ r_i$, where $r_i$ is the expected number of steps required to return to state $i$, if starting at state $i$.

\item If $Q^m$ is strictly positive (which implies aperiodic and recurrent) for some $m$, then

$$P(X_n = i) \to s_i \text{ as } n \to \infty$$


\end{enumerate}

\end{document}